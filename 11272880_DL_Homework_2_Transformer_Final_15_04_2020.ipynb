{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11272880-DL Homework 2 Transformer Final - 15-04-2020.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7weS5NZdrrn",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning II: Deep Learning and Applications\n",
        "\n",
        "# Homework 2\n",
        "\n",
        "**Due date: April 15**\n",
        "\n",
        "### Instructions\n",
        "\n",
        "The dataset is constructed from the DBLP database, where we collect a set of papers from five different research domains (i.e., machine learning, natural language processing, data mining, database, programming language). The goal is to predict the categories of papers. We have provided several files, and below is a description.\n",
        "\n",
        "1. train.csv. This file provides a number of paper ids and the corresponding labels, which can be used for training.\n",
        "\n",
        "2. test.csv. This file provides the paper ids for evaluation.\n",
        "\n",
        "3. text.csv. This file provides the title of each paper, which allows you to do prediction based on those titles.\n",
        "\n",
        "4. reference.csv. This file provides the references of each paper. Each line contains two ids, i.e., id_1, id_2, meaning that the paper id_1 cites the paper id_2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMjUwcJ15jq5",
        "colab_type": "text"
      },
      "source": [
        "### Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRR7te_X5geo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q torch skorch torchvision torchtext"
      ],
      "execution_count": 0,
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xrt7lynZ5iiE",
        "colab_type": "code",
        "outputId": "3cda928e-6c0e-4505-c0e0-102dbdaaaea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRbryV9VnMr9",
        "colab_type": "code",
        "outputId": "0054d49f-37c7-4b27-a041-0fe0a1494bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FckdrStHHzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext.vocab import Vectors\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuywP2WD5yrW",
        "colab_type": "text"
      },
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouYROQ1a5w1D",
        "colab_type": "code",
        "outputId": "01d6e401-3e9a-4e4a-c05c-a22bdfe07fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# We first download the repo to get access to data and some utility code (This is specifically for colab.)\n",
        "!rm -rf deep-learning/\n",
        "!git clone https://github.com/audreyanneguindon/deep-learning/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 19 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJYy9V3052JO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "ROOT_DIR='deep-learning/'\n",
        "DATA_DIR=os.path.join(ROOT_DIR, 'data/') # this is where most of the data lives"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XXJIzMJXK-D",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Transformers for Document Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UrYe8GTXN77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 28\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bert_name = 'bert-large-uncased'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6mTLyQhXRu8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f-BZjShtvds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_pandas_df(filename):\n",
        "    '''\n",
        "    Load the data into Pandas.DataFrame object\n",
        "    This will be used to convert data to torchtext object\n",
        "    '''\n",
        "    file = pd.read_csv(os.path.join(DATA_DIR, filename), sep=',')\n",
        "    return file\n",
        "\n",
        "def group_refs(i, ref_file):\n",
        "  ref_list = list(ref_file[ref_file.id_x==i][\"title\"])\n",
        "  return \". \".join(ref_list)\n",
        "\n",
        "def merge_data(text_file, data_file, ref_file, drop_id=True):\n",
        "  data_text = data_file.merge(text_file, how=\"inner\", on=\"id\")\n",
        "  ref_text = ref_file.merge(text_file, how=\"inner\", left_on=\"id.1\", right_on=\"id\")\n",
        "  cit_text = ref_file.merge(text_file, how=\"inner\", on=\"id\")\n",
        "  cit_text = cit_text.rename(columns = {'id.1':'id_x'})\n",
        "  cit_text = cit_text.rename(columns = {'id':'id.1'})\n",
        "  data_text[\"rtitle\"] = data_text.id.apply(lambda i: group_refs(i,ref_text))\n",
        "  data_text[\"ctitle\"] = data_text.id.apply(lambda i: group_refs(i,cit_text))\n",
        "  data_text[\"text\"] = data_text[\"title\"] + \". \" + data_text[\"rtitle\"] + \" \" + data_text[\"ctitle\"]\n",
        "  if drop_id:\n",
        "    data_text = data_text.drop(columns=[\"id\",\"title\",\"rtitle\",\"ctitle\"])\n",
        "  else:\n",
        "    data_text = data_text.drop(columns=[\"title\",\"rtitle\",\"ctitle\"])\n",
        "  return data_text\n",
        "\n",
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC7R-QJdu5bf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get datasets\n",
        "train_file = get_pandas_df('train.csv')\n",
        "test_file =  get_pandas_df('test.csv')\n",
        "text_file =  get_pandas_df('text.csv')\n",
        "ref_file =   get_pandas_df('reference.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezJvk5yHeoe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "from torchtext import data\n",
        "\n",
        "#Get BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_name)\n",
        "\n",
        "#Get special tokens\n",
        "init_token_idx = tokenizer.convert_tokens_to_ids(tokenizer.cls_token)\n",
        "eos_token_idx = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "\n",
        "#Get max sentence length\n",
        "max_input_length = tokenizer.max_model_input_sizes[bert_name]\n",
        "\n",
        "#\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(sequential = False, use_vocab = False)\n",
        "\n",
        "# Creating Field for data\n",
        "datafields = [(\"label\",LABEL),(\"text\",TEXT)]\n",
        "\n",
        "# \n",
        "train_df = merge_data(text_file, train_file, ref_file)\n",
        "train_df.to_csv('data.csv', index = None)\n",
        "\n",
        "#\n",
        "train_dataset = data.TabularDataset(path = 'data.csv',\n",
        "                                    format = 'csv',\n",
        "                                    fields = datafields,\n",
        "                                    skip_header = True )\n",
        "\n",
        "# Load data splits\n",
        "training, test_data = train_dataset.split(split_ratio = 0.6, random_state = random.seed(SEED))\n",
        "train_data, valid_data = training.split(split_ratio = 0.7, random_state = random.seed(SEED))\n",
        "\n",
        "# Build dictionary\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "# Build iterators\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch = True,\n",
        "    batch_size = 32, \n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_gLK6t9pc9D",
        "colab_type": "code",
        "outputId": "223bff9f-f621-4549-e7ff-08028df05941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of test examples: {len(test_data)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 5367\n",
            "Number of validation examples: 2300\n",
            "Number of test examples: 5112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIkEBaTcoUJ-",
        "colab_type": "code",
        "outputId": "40e89c81-a7b1-4d3f-993d-b64b61fb2b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "print('Vocab length')\n",
        "print(len(tokenizer.vocab))\n",
        "\n",
        "tokens = tokenizer.tokenize('dbms.') #database management system\n",
        "print(tokens)\n",
        "\n",
        "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(indexes)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(indexes)\n",
        "print(tokens)\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
        "print(max_input_length)\n",
        "\n",
        "print(vars(train_data.examples[6]))\n",
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
        "print(tokens)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab length\n",
            "30522\n",
            "['db', '##ms', '.']\n",
            "[16962, 5244, 1012]\n",
            "['db', '##ms', '.']\n",
            "101 102 0 100\n",
            "512\n",
            "{'label': '3', 'text': [23600, 2050, 2019, 6377, 2458, 4106, 1998, 22616, 4044, 2005, 6922, 2241, 3001, 1012, 1996, 2224, 1997, 2565, 18642, 19287, 1999, 4007, 3330, 1012, 2316, 6906, 14817, 2075, 10713, 2110, 4275, 2013, 9262, 3120, 3642, 1012, 7060, 1999, 3200, 15480, 2005, 10713, 2110, 22616, 1012, 23915, 2241, 12827, 10752, 1998, 22616, 1997, 26351, 8093, 10698, 9276, 1999, 16483, 3454, 12786, 2892, 12690, 3436, 10813, 1998, 9563, 5936, 1997, 5500, 2613, 2051, 1998, 11157, 3001, 3081, 7814, 8048, 2944, 5533, 4007, 2458, 1012, 20226, 19160, 1051, 2080, 22616, 2007, 8745, 7961, 1012, 2006, 8321, 6882, 22616, 1997, 10172, 4942, 29234, 4294, 2015]}\n",
            "['caden', '##a', 'an', 'integrated', 'development', 'analysis', 'and', 'verification', 'environment', 'for', 'component', 'based', 'systems', '.', 'the', 'use', 'of', 'program', 'dependence', 'graphs', 'in', 'software', 'engineering', '.', 'band', '##era', 'extract', '##ing', 'finite', 'state', 'models', 'from', 'java', 'source', 'code', '.', 'patterns', 'in', 'property', 'specifications', 'for', 'finite', 'state', 'verification', '.', 'invariant', 'based', 'specification', 'synthesis', 'and', 'verification', 'of', 'sync', '##hr', '##oni', '##zation', 'in', 'concurrent', 'programs', 'addressing', 'cross', '##cut', '##ting', 'deployment', 'and', 'configuration', 'concerns', 'of', 'distributed', 'real', 'time', 'and', 'embedded', 'systems', 'via', 'aspect', 'oriented', 'model', 'driven', 'software', 'development', '.', 'enhancing', 'modular', 'o', '##o', 'verification', 'with', 'separation', 'logic', '.', 'on', 'accurate', 'automatic', 'verification', 'of', 'publish', 'sub', '##scribe', 'architecture', '##s']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUfUetLzrqyA",
        "colab_type": "text"
      },
      "source": [
        "# Build the Model\n",
        "\n",
        "Next, we'll load the pre-trained model, making sure to load the same model as we did for the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXgVyulBrpeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained(bert_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9nKKD47r1XJ",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll define our actual model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_pClwj-rx9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTGRUClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = 0 if n_layers < 2 else dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, \n",
        "                             output_dim)\n",
        "  \n",
        "    def forward(self, text):                #text = [batch size, sent len] \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]   #embedded = [batch size, sent len, emb dim]\n",
        "        _, hidden = self.rnn(embedded)      #hidden = [n layers * n directions, batch size, emb dim]\n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])    #hidden = [batch size, hid dim]\n",
        "        output = self.out(hidden)                    #output = [batch size, out dim]\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQrDVJlQsBHX",
        "colab_type": "text"
      },
      "source": [
        "Next, we create an instance of our model using standard hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78xNfzfTr8ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 5\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = BERTGRUClassifier(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdQC9GfAsC-a",
        "colab_type": "text"
      },
      "source": [
        "We can check how many parameters the model has. Our standard models have under 5M, but this one has 112M! Luckily, 110M of these parameters are from the transformer and we will not be training those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnbxhgyusAdA",
        "colab_type": "code",
        "outputId": "56361d4a-8809-461d-ef28-5e64ae02e38f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 338,296,325 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-NbJFoBsIrg",
        "colab_type": "text"
      },
      "source": [
        "The model has 112,243,461 trainable parameters\n",
        "In order to freeze paramers (not train them) we need to set their requires_grad attribute to False. To do this, we simply loop through all of the named_parameters in our model and if they're a part of the bert transformer model, we set requires_grad = False."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiuVbxhhsG9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6rl94X8sOXl",
        "colab_type": "text"
      },
      "source": [
        "We can now see that our model has under 3M trainable parameters, making it almost comparable to the FastText model. However, the text still has to propagate through the transformer which causes training to take considerably longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wB_fWA2sQFG",
        "colab_type": "code",
        "outputId": "2ae764b8-7353-49b5-eb57-4f6dac8db7a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 3,154,437 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84u2y62QsU96",
        "colab_type": "text"
      },
      "source": [
        "We can double check the names of the trainable parameters, ensuring they make sense. As we can see, they are all the parameters of the GRU (rnn) and the linear layer (out)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlOXaGetwTUB",
        "colab_type": "code",
        "outputId": "e225ef13-bf99-4109-919f-7fa448b70259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 409 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 1024)\n",
            "bert.embeddings.position_embeddings.weight               (512, 1024)\n",
            "bert.embeddings.token_type_embeddings.weight               (2, 1024)\n",
            "bert.embeddings.LayerNorm.weight                             (1024,)\n",
            "bert.embeddings.LayerNorm.bias                               (1024,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight        (1024, 1024)\n",
            "bert.encoder.layer.0.attention.self.query.bias               (1024,)\n",
            "bert.encoder.layer.0.attention.self.key.weight          (1024, 1024)\n",
            "bert.encoder.layer.0.attention.self.key.bias                 (1024,)\n",
            "bert.encoder.layer.0.attention.self.value.weight        (1024, 1024)\n",
            "bert.encoder.layer.0.attention.self.value.bias               (1024,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight      (1024, 1024)\n",
            "bert.encoder.layer.0.attention.output.dense.bias             (1024,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight       (1024,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias         (1024,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight          (4096, 1024)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (4096,)\n",
            "bert.encoder.layer.0.output.dense.weight                (1024, 4096)\n",
            "bert.encoder.layer.0.output.dense.bias                       (1024,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                 (1024,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                   (1024,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "rnn.bias_ih_l1_reverse                                        (768,)\n",
            "rnn.bias_hh_l1_reverse                                        (768,)\n",
            "out.weight                                                  (5, 512)\n",
            "out.bias                                                        (5,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3m7Yf3MsZMX",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIpn7TrBZ_9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##training functions\n",
        "import time\n",
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
        "\n",
        "# Train a model\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss, epoch_acc = 0, 0 \n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "# Evaluate a model\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss, epoch_acc = 0, 0 \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RRXropLmUBL",
        "colab_type": "text"
      },
      "source": [
        "As is standard, we define our optimizer and criterion (loss function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdQ6LfRXsYXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MVpRq5pjupL",
        "colab_type": "text"
      },
      "source": [
        "Finally, we'll train our model. This takes considerably longer than any of the previous models due to the size of the transformer. Even though we are not training any of the transformer's parameters we still need to pass the data through the model which takes a considerable amount of time on a standard GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P65hZ6Ap3pw7",
        "colab_type": "code",
        "outputId": "cf782f82-2f4e-4d11-dd97-f67f836774ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "N_EPOCHS = 15\n",
        "\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "        \n",
        "    end_time = time.time()\n",
        "        \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.873 | Train Acc: 66.40%\n",
            "\t Val. Loss: 0.624 |  Val. Acc: 76.97%\n",
            "Epoch: 02 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.619 | Train Acc: 77.62%\n",
            "\t Val. Loss: 0.682 |  Val. Acc: 75.41%\n",
            "Epoch: 03 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.576 | Train Acc: 79.29%\n",
            "\t Val. Loss: 0.575 |  Val. Acc: 80.46%\n",
            "Epoch: 04 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.514 | Train Acc: 81.20%\n",
            "\t Val. Loss: 0.547 |  Val. Acc: 80.98%\n",
            "Epoch: 05 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.478 | Train Acc: 82.57%\n",
            "\t Val. Loss: 0.543 |  Val. Acc: 81.06%\n",
            "Epoch: 06 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.428 | Train Acc: 83.84%\n",
            "\t Val. Loss: 0.548 |  Val. Acc: 81.28%\n",
            "Epoch: 07 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.363 | Train Acc: 87.05%\n",
            "\t Val. Loss: 0.600 |  Val. Acc: 81.11%\n",
            "Epoch: 08 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.331 | Train Acc: 87.72%\n",
            "\t Val. Loss: 0.629 |  Val. Acc: 79.94%\n",
            "Epoch: 09 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.289 | Train Acc: 89.71%\n",
            "\t Val. Loss: 0.789 |  Val. Acc: 77.95%\n",
            "Epoch: 10 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.254 | Train Acc: 91.26%\n",
            "\t Val. Loss: 0.728 |  Val. Acc: 79.63%\n",
            "Epoch: 11 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.206 | Train Acc: 92.33%\n",
            "\t Val. Loss: 0.752 |  Val. Acc: 79.72%\n",
            "Epoch: 12 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.166 | Train Acc: 93.74%\n",
            "\t Val. Loss: 0.811 |  Val. Acc: 79.89%\n",
            "Epoch: 13 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.136 | Train Acc: 94.77%\n",
            "\t Val. Loss: 0.866 |  Val. Acc: 79.89%\n",
            "Epoch: 14 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.114 | Train Acc: 95.83%\n",
            "\t Val. Loss: 0.968 |  Val. Acc: 78.73%\n",
            "Epoch: 15 | Epoch Time: 1m 3s\n",
            "\tTrain Loss: 0.096 | Train Acc: 96.32%\n",
            "\t Val. Loss: 0.994 |  Val. Acc: 79.68%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2FrtsZJjg0a",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86d7xWAljeHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import torch.optim as optim\n",
        "# from transformers import AdamW\n",
        "\n",
        "# N_EPOCHS = 20\n",
        "# n_epochs_stop = 3\n",
        "# optimizers = [optim.Adam, optim.RMSprop]\n",
        "# learning_rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5] \n",
        "# dropout_rates = [0.25, 0.5, 0.75, 0.80]\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# criterion = criterion.to(device)\n",
        "# best_overall_valid_acc = 0.0\n",
        "\n",
        "# HIDDEN_DIM = 256\n",
        "# OUTPUT_DIM = 5\n",
        "# N_LAYERS = 2\n",
        "# BIDIRECTIONAL = True\n",
        "\n",
        "# for d in dropout_rates:\n",
        "#   for o in optimizers:\n",
        "#     for l in learning_rates:\n",
        "\n",
        "#       print('\\nTrain for: optimizer {0}, learning rate {1}, droupout rate {2}'.format(str(o), l, d))\n",
        "#       model = BERTGRUClassifier(bert,\n",
        "#                                 HIDDEN_DIM,\n",
        "#                                 OUTPUT_DIM,\n",
        "#                                 N_LAYERS,\n",
        "#                                 BIDIRECTIONAL,\n",
        "#                                 d)\n",
        "      \n",
        "#       for name, param in model.named_parameters():                \n",
        "#         if name.startswith('bert'):\n",
        "#           param.requires_grad = False\n",
        "\n",
        "#       optimizer = o(model.parameters(), lr=l)\n",
        "#       model = model.to(device)\n",
        "\n",
        "#       # train and test the model\n",
        "#       # DO NOT MODIFY\n",
        "#       best_valid_acc = 0.0\n",
        "#       for epoch in range(N_EPOCHS):\n",
        "        \n",
        "#         start_time = time.time()\n",
        "        \n",
        "#         train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "#         valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "        \n",
        "#         end_time = time.time()\n",
        "\n",
        "#         epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "#         if valid_acc > best_valid_acc:\n",
        "#             best_valid_acc = valid_acc\n",
        "#             epochs_no_improve = 0\n",
        "#             if valid_acc > best_overall_valid_acc:\n",
        "#               best_overall_valid_acc = valid_acc\n",
        "#               torch.save(model.state_dict(), 'best-model.pt')\n",
        "#         else:\n",
        "#           epochs_no_improve += 1\n",
        "#           # Check early stopping condition\n",
        "#           if epochs_no_improve == n_epochs_stop:\n",
        "#             print('Early stopping!')\n",
        "#             epochs_no_improve=0\n",
        "#             break\n",
        "\n",
        "#         print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "#         print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "#         print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "#   print('\\nTrain for: optimizer {0}, learning rate {1}'.format(str(o), l, ))\n",
        "#   print(best_overall_valid_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpL3NKqQjqQi",
        "colab_type": "text"
      },
      "source": [
        "# Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI4bi6G-wkAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db2e6083-60c4-431c-e28e-310a231f34dc"
      },
      "source": [
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.549 | Test Acc: 82.04%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv6veBxYwsoe",
        "colab_type": "text"
      },
      "source": [
        "# Kaggle Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODVpBz41kLoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_class(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    preds = model(tensor)\n",
        "    max_preds = preds.argmax(dim = 1)\n",
        "    return max_preds.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8qw52Gnwzzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = merge_data(text_file, test_file, ref_file, drop_id=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq5SRXxAYsd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = [[test_df.id[i], predict_class(model,tokenizer,test_df.text[i])] for i in range(len(test_df))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTdNBqhvYtdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df = pd.DataFrame(pred, columns = ['id', 'label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDmp0802aRa2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "545e2af5-20bf-46e0-c09a-03a9e8c2e610"
      },
      "source": [
        "pred_df"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12777</th>\n",
              "      <td>25553</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12778</th>\n",
              "      <td>25556</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12779</th>\n",
              "      <td>25558</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12780</th>\n",
              "      <td>25559</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12781</th>\n",
              "      <td>25560</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12782 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  label\n",
              "0          1      0\n",
              "1          2      1\n",
              "2          4      0\n",
              "3          5      2\n",
              "4          7      2\n",
              "...      ...    ...\n",
              "12777  25553      1\n",
              "12778  25556      3\n",
              "12779  25558      2\n",
              "12780  25559      2\n",
              "12781  25560      0\n",
              "\n",
              "[12782 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSCsTc87ZX9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df.to_csv(\"transformer_predictions_final.csv\", index = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-1m_CQ0J7G8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.to_csv(\"test_df.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsMOAWiPEmo-",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl_B9rm7EokZ",
        "colab_type": "text"
      },
      "source": [
        "Here we have a few sentences from the training dataset that are difficult to categorise even for people."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXEl-47Mzw_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"clustering association rules mining association rules between sets of items in large databases mining quantitative association rules in large relational tables sprint a scalable parallel classifier for data mining an interval classifier for database mining applications data \""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZR5oDTBzdoL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db67d17c-8918-4864-a8ff-566a3cac3c45"
      },
      "source": [
        "predict_class(model, tokenizer, text) #Label = 1"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEita0lDz2SK",
        "colab_type": "text"
      },
      "source": [
        "Human evaluation = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N3hPqDEjpTn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a937aee-9655-4846-8351-4907534157b9"
      },
      "source": [
        "predict_class(model, tokenizer, \"query evaluation and progression in aol knowledge bases\") #Label = 2"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MznAfymKyV5f",
        "colab_type": "text"
      },
      "source": [
        "Label = 2\n",
        "\n",
        "Human evaluation = 1 based on title (2 based on AI Conference)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKCfCryPFOWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0fe741f-0535-4904-f320-712a73ca79f8"
      },
      "source": [
        "predict_class(model, tokenizer, \"two directional record layout for multiple inheritance. an implementation of gem supporting a semantic data model on a relational back end. a recovery algorithm for a high performance memory resident database system\") #Label = 3"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNcI2IWcyZld",
        "colab_type": "text"
      },
      "source": [
        "Label = 3\n",
        "\n",
        "Human evaluation = 1 or 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBtS8ZF4FgNH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae1e0645-a152-4174-9c54-904d22cbec41"
      },
      "source": [
        "predict_class(model, tokenizer, \"psst a web based system for tracking political statements\") #Label = 0"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf19ckzQyhd5",
        "colab_type": "text"
      },
      "source": [
        "Label = 0\n",
        "\n",
        "Human evaluation = 0 or 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qc0X-30FoZu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b05fad59-96d0-4237-e621-ddabe6c0f1f9"
      },
      "source": [
        "predict_class(model, tokenizer, \"the locker metaphor to teach dynamic memory\") #Label = 3"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE8HeZpGyqbf",
        "colab_type": "text"
      },
      "source": [
        "Label = 3\n",
        "\n",
        "Human evaluation = 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZG-1FlexSjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01d17b28-46af-4c91-80ee-e5678bb80e2a"
      },
      "source": [
        "predict_class(model, tokenizer, \"nlp driven ir evaluating performances over a text classification task a re examination of text categorization methods training algorithms for linear text classifiers feature selection perceptron learning and a usability case study for text categorization context sensitive learning methods for text categorization \") #Label = 2"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYvs5mmHy3OW",
        "colab_type": "text"
      },
      "source": [
        "Label = 2\n",
        "\n",
        "Human evaluation = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwbu5j400nJW",
        "colab_type": "text"
      },
      "source": [
        "### Legend\n",
        "\n",
        "0 = data mining\n",
        "\n",
        "1 = database\n",
        "\n",
        "2 = machine learning \n",
        "\n",
        "3 = programming language\n",
        "\n",
        "4 = natural language processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJUSO0QIAa_E",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "The evaluation was used to understand how the model performed on the test set (split from the training data). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqQetYubAc_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df_2 = merge_data(text_file, train_file, ref_file, drop_id=False)\n",
        "X_train, test_data= train_test_split(train_df_2, test_size=0.20, random_state=random.seed(SEED))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kbOUn_VAjU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = test_data.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeSxGQ_7Akaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = [[test_data.id[i], test_data.label[i], predict_class(model,tokenizer,test_data.text[i])] for i in range(len(test_data))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EIzBJkbAqn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred_df = pd.DataFrame(test_pred, columns = ['id', 'test_label', 'pred_label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZharbER1Arsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred_df = test_pred_df.merge(train_df_2, how=\"inner\", on=\"id\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFIK2BhAAtRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred_df.to_csv(\"transformer_test_data.csv\", index=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2rCdzouA2PN",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "As part of this homework many models were tested. The models tested included a bidirectional LSTM, an LSTM with BERT transformer, a FastText model, and a SVM model. The LSTM achieved a score of 71.29% on the test set, the FastText model achieved a score of 76.80%, the BERT transformer with LSTM achieved a score of 79.28%, the SVM achieved a score of 81%, and the GRU with BERT transformer (bert-base uncased) achieved a score of 80%. The best model was the BERT transformer model using a GRU classifier, which achieved a score of 82.04%. Overall, combining the outputs of these models yielded the highest score at 82.60%. \n",
        "\n",
        "The models used were adapted from [bentrevett’s](https://github.com/bentrevett/pytorch-sentiment-analysis) [FastText model](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/3%20-%20Faster%20Sentiment%20Analysis.ipynb) and [Transformer model](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb). A hyper-parameter search and ablation study were conducted. The ablation study indicated that the best structure for the model has a hidden dimension of 256 (512) with 2 layers. BERT large which has 338,296,325 parameters reduced to 3,154,437 parameters outperformed the BERT base model which has 112,243,461 reduced to 2,761,221 parameters. We also tried to use attention with the LSTM models, but this did not yield significantly different results. Through the hyper-parameter search we found that the optimal dropout rate was between 0.5 and 0.75 and that the model performed best when trained using an Adam optimizer (lr=1-e3) for 15 epochs. \n",
        "\n",
        "The benefit of the BERT model is that it could be trained with less training data (5,367 examples) and still yield fairly good results. The model also did not require a lot of data engineering or pre-processing. However, to leverage the relationships between the papers, the text of the reference and citation were appended to the titles.\n",
        "\n",
        "The model still had difficulties with certain papers, especially with papers that had shorter titles and did not include references to other paper or were not refered to by other papers. In these cases the SVM model, which looks at term frequency tended to work better as the sentences did not have a lot of context. In the future, it would be useful to try and combine term frequency with the model to achieve better results.\n"
      ]
    }
  ]
}
